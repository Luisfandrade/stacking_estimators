{"cells":[{"metadata":{"_uuid":"866d01d0b8ec619f93dd7c45b759988ea859ab24"},"cell_type":"markdown","source":"# Titanic Stacking\nBy Luis andrade\n\n## Context\n* Stacking is the process by which several estimators are leveraged in order to produced a combined prediction that achieves the best performance\n* Stacking has been shown to perform well in a large variety of data science problems\n\n## Objectives\n* To implement a stacking of several well-known shallow estimators\n* To apply the stacking result to the Titanick survival problem and evaluate its performance\n\n\n## Dataset\n\nThe Dataset is the famous Titanick survival data, which contains information about the passengers that were onboard of the titanick. This dataset also includes whether the passenger survived or not, which in this case will be considered the target variable. \n\nThe dataset contains hundreds of passengers with a total of 12 fields"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bffc1ca4e5e6a0b779bce3dde2b2b44641d4588"},"cell_type":"code","source":"import sklearn\nimport warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97794cd84b7b0eca4f1cc32da56114801b9d67de"},"cell_type":"markdown","source":"## Part 1 - EDA\n\nIn this section some explanatory data analysis is carried out"},{"metadata":{"trusted":true,"_uuid":"6623d88fe6f05ff5bc8757eb7cdc2c9c79c03334"},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ae4a40e624ab70c30328ddafacb7a5bf6acadda"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb4ce79f626d77e8238ffaf5e4b4edd2c9366007"},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1b4449f93bf1c1d22d7a0abf2ec9ff4513993f8"},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwordcloud = WordCloud()\nwordcloud.generate(\" \".join(df[\"Name\"]))\n\nfig = plt.figure(figsize=(40, 30))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2- Feature Engineering\n\nWe have briefly examined our dataset, so let us know do some feature engineering to extract relevant features"},{"metadata":{"trusted":true,"_uuid":"3f220e64c4e0ef54502873450bbd284a51933274"},"cell_type":"code","source":"def get_title(text):\n    if \"Mr\" in text:\n        return 2\n    elif \"Miss\" in text:\n        return 0\n    elif \"Mrs\" in text:\n        return 1\n    else:\n        return -1\n    \ndef add_title_to_df(df_in):\n    \"\"\"Takes a Datarame df and adds a column title\"\"\"\n    df = df_in.copy()\n    df[\"title\"] = df[\"Name\"].apply(get_title)\n    df.loc[:, \"title\"]\n    return df\n\ndf_title = add_title_to_df(df)\ndf_title.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb529480d7c4fe6fecc1c777a4ed9a1b7b3e721"},"cell_type":"code","source":"def enrich_df(df_in):\n    \"\"\"Add columns sex_num, embarked_num and non-NAN Age to df\"\"\"\n    df = df_in.copy()\n    df[\"sex_num\"] = df[\"Sex\"].map({\"male\":1, \"female\":0})\n    df[\"embarked_num\"] = df[\"Embarked\"].factorize()[0]\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9db37f2f65ea5840a157aea1934d2b1963dc4755"},"cell_type":"code","source":"# It seems that out of the 12 columns there are 7 numeric \ndf_enriched = enrich_df(df_title)\ndf_enriched.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"671c78a3305ea7a8eda4f4dfedb937cc7c2e5377"},"cell_type":"code","source":"import seaborn as sns\ncorrelation = df_enriched.drop(\"PassengerId\", axis=1).corr().apply(abs)\nmask = np.zeros_like(correlation)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(correlation, mask=mask, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the correlation heatmap, it seems that Sex, Pclass, title, and Fare are the most correlated with the target variable"},{"metadata":{"trusted":true,"_uuid":"4a082ea7e73b4a66064870d60e21728f24fe2317"},"cell_type":"code","source":"correlation.loc[\"Survived\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40814c80fa11d49a4441d5c72c8839771e729462"},"cell_type":"code","source":"cols = correlation.columns.drop(\"Survived\")\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4d695e7541582e908994e355d11aafef59dc750"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer()\n\ndef df_to_X_and_y(df_in, cols, y_col=None):\n    \"\"\"Get X and y (if exists) from df\"\"\"\n    df = df_in.copy()\n    if y_col is not None:\n        y = df.loc[:, y_col]\n        X = df.drop(y_col, axis=1)[cols]\n    else:\n        y = None\n        X = df[cols]\n        \n    X_new = pd.DataFrame(imputer.fit_transform(X),\n                     index=X.index, columns=X.columns)  \n    # The imputer is an imputation transformer for missing values\n    \n    return X_new, y\n\nX, y = df_to_X_and_y(df_enriched, cols, \"Survived\")\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4283d92b7e5900977c6377e96b4313ed1915404"},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ad0cbd81e3422889228b20a31466710d70640e"},"cell_type":"markdown","source":"So we have our feature variables contained in matrix **X** and the target variable contained in vector $y$\n\n### Part 2b - Dimensionality Reduction \n\nSince we have our feature and target variables, let us do some dimensionality reduction to create new features "},{"metadata":{"trusted":true,"_uuid":"b691a20518c201febbb41e63127d7bd2fa864de7"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X)\nplt.plot(range(1, X.shape[1]+1), pca.explained_variance_ratio_)\nplt.title(\"Explained Variance Ratio Vs N components\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen by the above figure, the eplained variance greatly decreases after the third principal component. We can use this \"elbow\" to determine the number of principal components that we will use"},{"metadata":{"trusted":true,"_uuid":"56ce92951c205dec933bd6646262b661ff30ffba"},"cell_type":"code","source":"Z = PCA(n_components =3).fit_transform(X)\nZ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e1ac5d5ecfad29a078878e6cc1312df0c730b7f"},"cell_type":"code","source":"Z.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"879382c803ccc92ad1d51dc1914861417c64852f"},"cell_type":"markdown","source":"## Part 3 - Modeling\n\nSince we now have our features and target ready, we can start exploring with some models"},{"metadata":{"trusted":true,"_uuid":"5483a9f37c234d855b5daa6044182b17bf5ba5a5"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, cross_validate\nfrom sklearn.pipeline import Pipeline\n\nscaler = StandardScaler()\n\ndef compare_classifiers(cls_list, cv=5, params=None):\n    \"\"\"\n    This function compares different estimators\n    \n    Arguments\n    ---------\n    cls_list : list\n        list of estimator classes\n    cv : int, default=5\n        Number of folds to use in cross-validation\n    params : dict, default=None\n        Dict indexed by class name whose values are dicts\n        with the parameter values to feed to GridSearchCV\n        By default, no gridsearch is performed\n        \n    Returns\n    -------\n    df : Pandas DataFrame\n        Datarame containing the results of the estimators\n    \"\"\"\n    scores_dict ={}\n    for cls_ in cls_list:\n        if params is not None:\n            clf = cls_(n_estimators = params)\n        else:\n            clf = cls_()\n        scores = {}\n        pipeline = Pipeline([(\"scaler\", scaler), (cls_.__name__, clf)])\n        plain_score = cross_validate(clf, X, y, cv=cv)\n        scaled_X = cross_validate(pipeline, X, y, cv=cv)\n        scaled_Z = cross_validate(pipeline, Z, y, cv=cv)\n        scores[\"plain \"] = plain_score\n        scores[\"scaled_X \"] = scaled_X\n        scores[\"scaled_Z \"] = scaled_Z\n        scores_dict[cls_.__name__] = scores\n\n    df = pd.DataFrame(scores_dict)\n    return df.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f7492d55ca584be82fa6f9b7e2aa0ff0c23301"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\ncls_list = [LogisticRegression, BernoulliNB, KNeighborsClassifier,\n            SVC, DecisionTreeClassifier]\n\ndf = compare_classifiers(cls_list)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c31b18a18a4b2785affd8fa4e897564b21a97d46"},"cell_type":"code","source":"def show_heatmap(df, col_name):\n    df_scores = df.applymap(lambda x : np.mean(x[col_name]))\n    sns.heatmap(df_scores, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa10da1f8a45fdf75371fea5504f02c0f8b01f6"},"cell_type":"code","source":"def show_model_times(df):\n    df_times = df.applymap(lambda x : sum(x[\"fit_time\"]) + sum(x[\"score_time\"]))\n    sns.heatmap(df_times, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c101d0f90cf1d4ad801df3a2b9966ef71490c1e","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import(AdaBoostClassifier, \n                             GradientBoostingClassifier,\n                             BaggingClassifier,\n                             ExtraTreesClassifier,\n                             RandomForestClassifier)\n\nensemble_cls_list = [AdaBoostClassifier, \n                     GradientBoostingClassifier,\n                     BaggingClassifier,\n                     ExtraTreesClassifier,\n                     RandomForestClassifier]\n\nensemble_df = compare_classifiers(ensemble_cls_list, cv=5, params=150)\nensemble_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb969e48c32f18019604f3b5bd761ea993fa7c87"},"cell_type":"code","source":"concat_df = pd.concat([df, ensemble_df])\nconcat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99e1429aa510cb9cce6005840be7ca04a4b19512"},"cell_type":"code","source":"show_heatmap(concat_df, \"test_score\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b8f229746f59a2752da728e30a2c5ac28eff7f7","trusted":true},"cell_type":"code","source":"show_model_times(concat_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9722fd8847960509873f285029f3c289ed2795a0"},"cell_type":"markdown","source":"### GridSearchCV\n\nNow let us use grid search CV to find the best parameters for each of the estimators"},{"metadata":{"trusted":true,"_uuid":"5f8d9979340ff44e1e2e79691d810132194a844d"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef compare_gridsearch_classifiers(cls_list, X, y, cv=5, params=None, verbose=0):\n    \"\"\"\n    This function compares different estimators\n    \n    Arguments\n    ---------\n    cls_list : list\n        list of estimator classes\n    X : pandas DataFrame\n        Feature matrix\n    y : pandas Series or array\n        Target vector\n    cv : int, default=5\n        Number of folds to use in cross-validation\n    params : dict, default=None\n        Dict indexed by class name whose values are dicts\n        with the parameter values to feed to GridSearchCV\n        By default, no gridsearch is performed\n    Verbose : int, default=0\n        Control Verbose level\n        \n    Returns\n    -------\n    df : Pandas DataFrame\n        Datarame containing the results of the estimators\n    \"\"\"\n    scores_dict ={}\n    best_params_dict={}\n    for cls_ in cls_list:\n        print(f\"testing {cls_}\")\n        clf = cls_()\n        scores = {}\n        clf_name = cls_.__name__\n        clf_params = params.get(clf_name)\n        if clf_params is not None:\n            pipeline = Pipeline([(\"scaler\", scaler), (clf_name, clf)])\n            gridsearch_plain = GridSearchCV(clf, clf_params, cv=cv, verbose=verbose)\n            clf_params = {\"__\".join([clf_name, key]):value for key, value in clf_params.items()}\n            gridsearch_scaled = GridSearchCV(pipeline, clf_params, cv=cv, verbose=verbose)\n            scores[\"plain\"] = gridsearch_plain.fit(X,y)\n            scores[\"scaled_X \"] = gridsearch_scaled.fit(X,y)\n            #scores[\"scaled_Z \"] = gridsearch_scaled.fit(Z, y).score(Z, y)\n            scores_dict[clf_name] = scores\n\n    return scores_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"799952ca0136914ccd8988847f6e46fce61ef364"},"cell_type":"code","source":"ensemble_cls_list ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36aac64ced81edca1f3fe5cedcfc7519ce9557dd"},"cell_type":"code","source":"standard_ensemble_dict = {\"n_estimators\":np.linspace(50, 300, 6, dtype=np.int16),\n                         \"learning_rate\": np.linspace(0.01, 2, 5)}\nparameters = {\n    LogisticRegression.__name__:{\"tol\":[1e-3, 1e-4], \"C\": [0.1, 0.5, 1.0]},\n    BernoulliNB.__name__:{\"alpha\": [0.1, 0.5, 1], \"binarize\": [0.0, 0.5], \"fit_prior\": [True, False]},\n    KNeighborsClassifier.__name__:{\"n_neighbors\": [1, 3, 5, 10], \"weights\": [\"uniform\", \"distance\"]},\n    SVC.__name__:{\"C\": [0.1, 0.5, 1.0], \"kernel\": [\"linear\", \"rbf\", \"sigmoid\"]},\n    DecisionTreeClassifier.__name__:{\"criterion\": [\"gini\", \"entropy\"], \"splitter\":[\"best\", \"random\"]},\n    AdaBoostClassifier.__name__:{**standard_ensemble_dict},\n    GradientBoostingClassifier.__name__:{**standard_ensemble_dict},\n    BaggingClassifier.__name__:{},\n    ExtraTreesClassifier.__name__:{\"n_estimators\": [100]},\n    }\n\nparameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46979f5d98dcb535ebdee673e70756496f52e194"},"cell_type":"code","source":"scores_dict = compare_gridsearch_classifiers(cls_list, X, y, cv=5, params=parameters, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c5dac63938344e9a84544f4bfb295e65f57bf5"},"cell_type":"code","source":"ensemble_scores_dict = compare_gridsearch_classifiers(ensemble_cls_list, X, y, cv=5, params=parameters, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab0195ecae291b7097be04cd8228642c6d3e46dd"},"cell_type":"code","source":"scores_df = pd.DataFrame({**scores_dict, **ensemble_scores_dict})\nscores_df.T ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f43426a2c0754be09f88f21859d78e99c6ce13e4"},"cell_type":"code","source":"best_scores_df = scores_df.applymap(lambda x: x.best_score_)\nsns.heatmap(best_scores_df.T, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that some estimators have a dev set performance of up to 83%, such as the SVC and the Gradient Boosting Classifier"},{"metadata":{"trusted":true,"_uuid":"421cadf4930b4ad6890593406189b67acc67aad1"},"cell_type":"code","source":"scores_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d44818857cc946720455fa7546d48d0e05579567"},"cell_type":"code","source":"scaled_X_objs = scores_df.loc[\"scaled_X \",:]\npreds = scaled_X_objs.apply(lambda x: x.best_estimator_.predict(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"710ab281cc767b78a0abb19890cef879ea92c638"},"cell_type":"code","source":"preds_corr_df = pd.DataFrame.from_items(zip(preds.index, preds.values)).corr()\nsns.heatmap(preds_corr_df, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are considering to use stacking it is also usefull to look at the correlation between the predictions of the different estimators. We want to stack models that whose predictions are not too correlated, otherwise the stacking may end up being redundant. Looking at the heatmap there seems to be two groups of \"similar\" estimators: the tree emsemble group and the remaining estimators. It would be interesting to use at least one estimator from each group in the stacking"},{"metadata":{"trusted":true,"_uuid":"eb2f61c362859e30749b4df62061177a4c51dad2"},"cell_type":"markdown","source":"## Part 4 - Stacking\n\nWe have looked and compared several estimators and tuned their hyperparameters, but can we get a better performance by using stacking?\nLet's find out\n\nThe approach chosen here is to have a two-layer stacking, with three classifiers on the first layer and a \"meta\" classifier on the second layer"},{"metadata":{"trusted":true,"_uuid":"306c8deae4e99ec7b1ae687674b11230a18c6e74"},"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier # Library to help with the stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44190ffdcd8564be2ccefa8c172b22260143330a"},"cell_type":"code","source":"# Let us chose the best tree emsemble estimator, the best non tree-ensemble estimator, and a third estimator (just to break the tie)\nclf1= scaled_X_objs[SVC.__name__].best_estimator_ # best non tree-ensemble estimator\nclf2= scaled_X_objs[GradientBoostingClassifier.__name__].best_estimator_ # best tree-ensemble estimator\nclf3= scaled_X_objs[BernoulliNB.__name__].best_estimator_# third estimator to break the tie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c707f15ed9c48b1172ac250cc911b31d99cccbe5"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nstacking_scores = {}\n\n# Do the actual stacking, iterating through a list of metaclassifiers\nfor lr in [*ensemble_cls_list, *cls_list]:\n    sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],\n                                meta_classifier=lr())\n    stacking_scores[lr.__name__] = np.mean(cross_val_score(sclf, X.values, y.values))   \n\npd.Series(stacking_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that using the Random Forest Classifer as the meta classifier we manage to slightly outperform the single best model"},{"metadata":{"trusted":true,"_uuid":"6b21659d7e200c7fe20a63697fc001b966e03844"},"cell_type":"markdown","source":"## Part 5 - Making a Submission\n\nFinnally, le us make a prediction to see how well our stacked classifier does on the leaderboard"},{"metadata":{"trusted":true,"_uuid":"dcd6afd6bf97c10f3833b38007d70fd8dc918959"},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\nX_test, _ = df_to_X_and_y(enrich_df(add_title_to_df(df_test)), cols, y_col=None)\nfinal_clf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=SVC())\nfinal_clf.fit(X.values, y.values)\npreds = final_clf.predict(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61ea838c22fc63e5e879f257c06af83a56d8cd14"},"cell_type":"code","source":"submission_df = pd.DataFrame({\"Survived\":preds }, index=df_test[\"PassengerId\"])\nsubmission_df.to_csv(\"submission.csv\")\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a61fa723a9f25ec22dce50124cdc36397d402a2"},"cell_type":"markdown","source":"## Part 6 - Conclusion\n\nTo recap, this notebook started with a simple EDA to explore the dataset. Then, feature engineering was carried out to produce the desired variables. After this, a set of estimators were hyperparameter-tuned and and their performance was compared. Afterwards, a set of these estimators were used in the first layer of the stack along with a metaclassifier on the second layer. The best performance was obtained by one of the stacked models"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}